{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ce7f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.path as mpath\n",
    "import scipy.sparse as sp\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.interpolate import LinearNDInterpolator, CloughTocher2DInterpolator, CubicSpline, interp1d, PchipInterpolator, RegularGridInterpolator\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from spektral.data import Dataset, Graph\n",
    "from spektral.data.loaders import DisjointLoader, SingleLoader\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from spektral.layers import MessagePassing, GCNConv, GATConv, ECCConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from codebase.gnn_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 20\n",
    "encoder_data_history_range = (-40,0+1,10)\n",
    "file_path = r'datasets/4000.am'\n",
    "\n",
    "slices = [slice(0,1001,1),slice(0,250,1),slice(250,500,1)]\n",
    "data_train = loadData(file_path, np.arange(*encoder_data_history_range), gradsBC=True, slices=slices, sigma=sigma)\n",
    "meshing_size = 0.1\n",
    "data_train_remesh = [RemeshData(data_train[i],meshing_size) for i in range(0,len(data_train),30)]\n",
    "slices_test = [slice(0,1001,1),slice(250,500,1),slice(250,500,1)]\n",
    "data_test = loadData(file_path, np.arange(*encoder_data_history_range), gradsBC=True, slices=slices_test, sigma=sigma)\n",
    "data_test_remesh = [RemeshData(data_test[i],meshing_size) for i in range(0,len(data_test),100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d35697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created cache directory: data\n",
      "Generating 33 new graph samples...\n",
      "Caching 33 graphs to data/cfd_dynamic_train_graphs_setup1.pkl\n",
      "Generating 10 new graph samples...\n",
      "Caching 10 graphs to data/cfd_dynamic_test_graphs_setup1.pkl\n",
      "Starting feature normalization...\n",
      "Node feature global mean (from train_dataset, shape (23,)): [-0.18464912  0.35898146  0.15273103 -0.28329974 -0.22453131  0.3793628\n",
      "  0.16170157 -0.29028523 -0.25424328  0.3951998   0.16979711 -0.29773837\n",
      " -0.27259547  0.41457883  0.17801748 -0.30564582 -0.30557045  0.42995834\n",
      "  0.18568549 -0.3140693  -0.01706287 -0.00508426  0.2777778 ]\n",
      "Node feature global std (from train_dataset, shape (23,)): [2.1612613  1.5076146  0.6517483  0.7138889  2.1600606  1.4946195\n",
      " 0.6546287  0.71063995 2.1623158  1.4915944  0.657476   0.70789194\n",
      " 2.1617823  1.472611   0.6601113  0.70593685 2.15482    1.4552897\n",
      " 0.6623511  0.70429516 0.5454589  0.54432905 0.44789693]\n",
      "Training node features normalized.\n",
      "Validation node features normalized.\n",
      "Edge feature global mean (from train_dataset, shape (3,)): [8.8873579e-12 1.1607978e-12 9.7351409e-02]\n",
      "Edge feature global std (from train_dataset, shape (3,)): [0.06897345 0.06918119 0.00813528]\n",
      "Training edge features normalized.\n",
      "Validation edge features normalized.\n",
      "Feature normalization complete.\n"
     ]
    }
   ],
   "source": [
    "# Generate graph datasets:\n",
    "dataset_train = FEMDataset(data_train_remesh,cache_file=\"data/cfd_dynamic_train_graphs_setup1.pkl\",force_regenerate=True)\n",
    "dataset_val = FEMDataset(data_test_remesh,cache_file=\"data/cfd_dynamic_test_graphs_setup1.pkl\",force_regenerate=True)\n",
    "\n",
    "# Normalize graph datasets:\n",
    "x_mean, x_std, e_mean, e_std = normalize_dataset_features(dataset_train, dataset_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f3628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d7/0s3_ltgs6476j7ypg4tch4xr0000gn/T/ipykernel_80709/3219315233.py:85: UserWarning: you are shuffling a 'FEMDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(dataset_train) # Shuffle the actual list of graph objects\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# --- Determine Fixed Feature Dimensions ---\n",
    "sample_graph = dataset_train[0]\n",
    "F_x_dim = sample_graph.x.shape[1]\n",
    "F_e_dim = sample_graph.e.shape[1]\n",
    "F_y_dim = sample_graph.y.shape[1]\n",
    "D_nodes_dim = sample_graph.fem_nodes.shape[1]\n",
    "nodes_per_element_dim = sample_graph.fem_elements.shape[1]\n",
    "\n",
    "# --- Define Input Signatures ---\n",
    "common_input_signature = [\n",
    "    tf.TensorSpec(shape=[None, F_x_dim], dtype=tf.float32, name=\"x\"),\n",
    "    tf.SparseTensorSpec(shape=[None, None], dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=[None, F_e_dim], dtype=tf.float32, name=\"e\"), # Shape [0, F_e_dim] if no edges/features\n",
    "    tf.TensorSpec(shape=[None, F_y_dim], dtype=tf.float32, name=\"y\"),\n",
    "    {\n",
    "        \"nodes\": tf.TensorSpec(shape=[None, D_nodes_dim], dtype=tf.float32, name=\"mesh_nodes\"),\n",
    "        \"elements\": tf.TensorSpec(shape=[None, nodes_per_element_dim], dtype=tf.int32, name=\"mesh_elements\"),\n",
    "        \"boundaryNodes\": tf.TensorSpec(shape=[None], dtype=tf.int32, name=\"mesh_boundaryNodes\"),\n",
    "        \"internalNodes\": tf.TensorSpec(shape=[None], dtype=tf.int32, name=\"mesh_internalNodes\"),\n",
    "        \"edge_vector\": tf.TensorSpec(shape=[None, 2], dtype=tf.float32, name=\"mesh_edge_vector\"),\n",
    "        \"edge_length\": tf.TensorSpec(shape=[None, 1], dtype=tf.float32, name=\"mesh_edge_length\"),\n",
    "    }\n",
    "]\n",
    "\n",
    "# --- Create Model, Loss, Optimizer ---\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.3e-3) # Use learning_rate\n",
    "n_dims = 64\n",
    "model = FEMGNN(hidden_dim=n_dims, n_gnn_layers=4, out_dim=F_y_dim, ecc_conv_kernel_network=[n_dims,n_dims])\n",
    "n_epochs = 600\n",
    "history = {'loss': [], 'val_loss': []}\n",
    "\n",
    "\n",
    "# --- tf.function decorated training step ---\n",
    "@tf.function(input_signature=common_input_signature)\n",
    "def train_step(x_in, a_in, e_in, y_in, mesh_in):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Note: The model receives the dictionary directly as its first argument.\n",
    "        # Ensure FEMGNN's call method expects: model_inputs, training=True\n",
    "        # where model_inputs = {\"x\": x_in, \"a\": a_in, \"e\": e_in, \"mesh\": mesh_in}\n",
    "        predictions = model({\"x\": x_in, \"a\": a_in, \"e\": e_in, \"mesh\": mesh_in}, training=True)\n",
    "        loss = loss_fn(y_in, predictions)\n",
    "    \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# --- tf.function decorated evaluation step ---\n",
    "@tf.function(input_signature=common_input_signature)\n",
    "def evaluate_step(x_in, a_in, e_in, y_in, mesh_in):\n",
    "    predictions = model({\"x\": x_in, \"a\": a_in, \"e\": e_in, \"mesh\": mesh_in}, training=False)\n",
    "    loss = loss_fn(y_in, predictions)\n",
    "    return loss\n",
    "\n",
    "# --- Helper function to convert graph object to tensors ---\n",
    "def prepare_graph_inputs(g_object, expected_F_e_dim):\n",
    "    x_tensor = tf.convert_to_tensor(g_object.x, dtype=tf.float32)\n",
    "    y_tensor = tf.convert_to_tensor(g_object.y, dtype=tf.float32)\n",
    "    e_tensor = tf.convert_to_tensor(g_object.e, dtype=tf.float32)\n",
    "\n",
    "    # Handle adjacency matrix\n",
    "    A_csr = g_object.a.tocsr()\n",
    "    A_coo = A_csr.tocoo()\n",
    "    indices = np.vstack((A_coo.row, A_coo.col)).T\n",
    "    a_tf_sparse = tf.SparseTensor(indices=indices, values=A_coo.data, dense_shape=A_coo.shape)\n",
    "    a_tf_sparse = tf.sparse.reorder(a_tf_sparse)\n",
    "\n",
    "    mesh_dict_tf = {\n",
    "        \"nodes\": tf.convert_to_tensor(g_object.fem_nodes, tf.float32),\n",
    "        \"elements\": tf.convert_to_tensor(g_object.fem_elements, tf.int32),\n",
    "        \"boundaryNodes\": tf.convert_to_tensor(g_object.fem_boundaryNodes, tf.int32),\n",
    "        \"internalNodes\": tf.convert_to_tensor(g_object.fem_internalNodes, tf.int32),\n",
    "        \"edge_vector\": tf.convert_to_tensor(g_object.fem_edge_vector, tf.float32),\n",
    "        \"edge_length\": tf.convert_to_tensor(g_object.fem_edge_length, tf.float32),\n",
    "    }\n",
    "    return x_tensor, a_tf_sparse, e_tensor, y_tensor, mesh_dict_tf\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    epoch_loss_sum = 0.0\n",
    "    np.random.shuffle(dataset_train) # Shuffle the actual list of graph objects\n",
    "    dataset_train_sub = dataset_train[::] # Or however you subset\n",
    "\n",
    "    for i, g in enumerate(dataset_train_sub):\n",
    "        x_g, a_g, e_g, y_g, mesh_g = prepare_graph_inputs(g, F_e_dim)\n",
    "        loss_val = train_step(x_g, a_g, e_g, y_g, mesh_g)\n",
    "        epoch_loss_sum += loss_val.numpy() # Add .numpy() to get Python float\n",
    "        current_avg_loss = epoch_loss_sum / (i + 1)\n",
    "        print(f\"\\rEpoch {epoch}/{n_epochs};  Iteration = {i+1}/{len(dataset_train_sub)};  Loss = {current_avg_loss:.4g}\", end='')\n",
    "    \n",
    "    avg_epoch_loss = epoch_loss_sum / len(dataset_train_sub)\n",
    "    history['loss'].append(avg_epoch_loss)\n",
    "\n",
    "    val_loss_sum = 0.0\n",
    "    for g_val in dataset_val:\n",
    "        x_v, a_v, e_v, y_v, mesh_v = prepare_graph_inputs(g_val, F_e_dim)\n",
    "        loss_v = evaluate_step(x_v, a_v, e_v, y_v, mesh_v)\n",
    "        val_loss_sum += loss_v.numpy()\n",
    "    \n",
    "    avg_val_loss = val_loss_sum / len(dataset_val)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "    print(f\";  Val Loss = {avg_val_loss:.4g}\")\n",
    "\n",
    "# --- Plot Training History ---s\n",
    "plt.plot(history['loss'],label='loss')\n",
    "plt.plot(history['val_loss'],label='val_loss')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe37fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_eval_remesh = [RemeshData(data_test[i],meshing_size) for i in range(0,len(data_test),5)]\n",
    "dataset_eval = FEMDataset(data_eval_remesh,cache_file=\"data/cfd_dynamic_test_graphs_setup1.pkl\",force_regenerate=True)\n",
    "dataset_train = FEMDataset(data_train_remesh,cache_file=\"data/cfd_dynamic_train_graphs_setup1.pkl\",force_regenerate=True)\n",
    "x_mean, x_std, e_mean, e_std = normalize_dataset_features(dataset_train, dataset_eval)\n",
    "\n",
    "val_loss_sum = 0.0\n",
    "eval_results = []\n",
    "for g_eval in dataset_eval:\n",
    "    x_v, a_v, e_v, y_v, mesh_v = prepare_graph_inputs(g_eval, F_e_dim)\n",
    "    predictions = model({\"x\": x_v, \"a\": a_v, \"e\": e_v, \"mesh\": mesh_v}, training=False)\n",
    "    loss = loss_fn(y_v, predictions)\n",
    "    eval_results.append({\n",
    "        'mesh': mesh_v,\n",
    "        'predictions': predictions.numpy(),\n",
    "        'ground_truth': y_v.numpy(),\n",
    "        'loss': loss.numpy(),\n",
    "        'err': predictions.numpy()-y_v.numpy(),\n",
    "    })\n",
    "\n",
    "err = np.array([d['loss'] for d in eval_results])\n",
    "plt.plot(err)\n",
    "plt.xlabels('timestep id')\n",
    "plt.ylabels('MSE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".gnnvenv (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
